\documentclass[12pt]{article}
\usepackage{amsmath}
\title{EECS 440: Progamming Assignment 2}
\author{Justin Gray}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\usepackage{graphicx}
\usepackage{float}

\begin{document}

\maketitle

a) Below is the data from the 5 experiments, run with no hidden nodes and $\gamma$=0. The data sets were run with 
5-fold cross validation and all results (except AROC) averaged. 
\begin{table}[ht!]
    \begin{tabular}{|c|c|c|c|c|c|} \hline
                             & ab            & cr            & sp              & vo              & ye\\ \hline
    Accuracy ($\mu$,$\sigma$)& (0.586,0.054) & (0.698,0.161) & (0.915,0.008)   & (0.820,0.010)   &(0.689,0.021)\\ \hline
    Precision($\mu$,$\sigma$)& (0.660,0.166) & (0.697,0.184) & (0.892,0.011)   & (0.689,0.349)   &(0.489,0.035)\\ \hline
    Recall   ($\mu$,$\sigma$)& (0.671,0.256) & (0.803,0.117) & (0.892,0.014)   & (0.646,0.346)   &(0.457,0.065)\\ \hline  
    AROC                     &  0.385        &  0.673        & 0.953           & 0.898           & 0.697 \\
    \hline
    \end{tabular}
\end{table}

The training times fore each experiment varried, even between folds. Compared to the decision stump analysis on the same 
datasets, the ANN was more accurate for almost all the datasets. For the ye data, the decision stump was more accurate. For the 
ab data, the stumps were considerably more accurate. 

b) Experiments on the vo and cr data sets, with training limits of:  100,   2575,   5050,   7525,  10000

\begin{table}[ht!]
    \begin{tabular}{|c|c|c|c|c|c|} 
    \hline
    iterations & 100   & 2575  & 5050  & 7525  & 10000 \\ \hline
    aroc       & 0.941 & 0.976 & 0.977 & 0.976 & 0.975  \\
    \hline
    \end{tabular}
    \caption{Data for vo experiments}
\end{table}

Compared to the perceptron, the aroc of vo ann was increased, even just training for 100 iterations. 
However it took much longer (almost 10 times longer) to converge to a steady answer. Additionally, 
you can notice that the aroc started to decrease as the interations went up, but very slowly. This was 
accompanied by a fall in accuracy, indicating that overfitting was starting to occur.

\begin{table}[ht!]
    \begin{tabular}{|c|c|c|c|c|c|} 
    \hline
    iterations & 100   & 2575  & 5050  & 7525  & 10000 \\ \hline
    aroc       & 0.896 & 0.902 & 0.902 & 0.902 & 0.902  \\
    \hline
    \end{tabular}
    \caption{Data for cr experiments}
\end{table}

Once again, the accuracy of this model increased with a hidden node layer. For this set, the training iterations
did not significantly increase though. The perceptron model also took a long time to train. You can see rapid convergence
and leveling off of the AROC as iterations increased. The model seemed to be training extremely slowly after about 2000 iterations.

c) Experiments were again completed on the vo and cr datasets: 

\begin{table}[!hb]
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
        & 5   &  6  &  7  &  8  &  9  &  10  & 15   \\ \hline \hline
      vo&0.975&0.978&0.970&0.974&0.966&0.981 & 0.983\\ \hline
      cr&0.902&0.901&0.899&0.897&0.901&0.898 &0.896 \\ \hline
    \end{tabular}
\end{table}

For both datasets, the AROC as compared to the experiments in part are not significantly 
different. Increasing the number of hidden nodes seems to flatline the AROC showing not 
much incremental benefit. 

For the most part these results agree well with expecataions. The AROC is much 
higher than the values found for the perceptron, indicating a better fit. However, 
increasing the number of nodes as well as the number of training iterations does not seem to 
have a strong impact on over fititng. there is som noise in the data. 

The results from all of these experiments seem to indicate that averaging model results over 
a n-fold cross validation (assuming n is reasonably large), when combined with the weight 
decay modification for training, provides a reasonable defence against overfitting. 

\end{document}