\documentclass[12pt]{article}
\usepackage{amsmath}
\title{EECS 440: HW 6}
\author{Justin Gray}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\usepackage{graphicx}
\usepackage{float}

\begin{document}

\maketitle
\setcounter{equation}{0}
1) 

i) For $K = aK_1 + bK_2$, since $K_1$ and $K_2$ are both valid kernels, then each K must 
have an associated $\psi$, called $\psi_1$ and $\psi_2$. Using mercer's conditions 
we can show that K is a valid kernel. 

Symmetry: 
\begin{equation}
    K(x_1,x_2) = K(x_2,x_1)
\end{equation}
\begin{equation}
    a\psi_1(x_1)\cdot\psi_1(x_2) + b\psi_2(x_1)\cdot\psi_2(x_2) = a\psi_1(x_2)\cdot\psi_1(x_1) + b\psi_2(x_2)\cdot\psi_2(x_1)
\end{equation}
Since the dot product is not order dependent: 
\begin{align}
    a\psi_1(x_1)\cdot\psi_1(x_2) = a\psi_1(x_2)\cdot\psi_1(x_1) \notag \\
    b\psi_2(x_1)\cdot\psi_2(x_2) = b\psi_2(x_2)\cdot\psi_2(x_1)
\end{align}
With a and b both being positive constants, symmetry holds. 

Positive SemiDefinite: 
\begin{equation}
    v^T K v \geq 0, \forall v\neq 0
\end{equation}

\begin{equation}
    v^T K v = (v^T aK_1+ bK_2) v
\end{equation}

Since K is symmetric, 
\begin{align}
    v^T aK_1K_2 v =& \sum_i\sum_jv_iv_j \left(a(\psi_1(x_i) \cdot \psi_1(x_j))+ b(\psi_2(x_i) \cdot \psi_2(x_j))\right) \notag \\
    =& a \sum_i\sum_jv_iv_j (\psi_1(x_i) \cdot \psi_1(x_j))+ b\sum_i\sum_jv_iv_j(\psi_2(x_i) \cdot \psi_2(x_j)) \notag \\
    =& a \parallel \sum_iv_i \psi_1(x_i)\parallel_2^2 + b\parallel\sum_iv_i \psi_2(x_i)\parallel_2^2
\end{align}

Thus
\begin{equation}
    a \parallel \sum_iv_i \psi_1(x_i)\parallel_2^2 + b\parallel\sum_iv_i \psi_2(x_i)\parallel_2^2 \geq 0
\end{equation}
for all positive values of a,b K will be positive semidefinite




ii) For $K = aK_1K_2$, since $K_1$ and $K_2$ are both valid kernels, then each K must 
have an associated $\psi$, called $\psi_1$ and $\psi_2$. Using mercer's conditions 
we can show that K is a valid kernel. 

Symmetry: 
\begin{equation}
    K(x_1,x_2) = K(x_2,x_1)
\end{equation}
\begin{equation}
    a\left(\psi_1(x_1)\cdot\psi_1(x_2)\right)\left(\psi_2(x_1)\cdot\psi_2(x_2)\right) 
    = a\left(\psi_1(x_2)\cdot\psi_1(x_1)\right)\left(\psi_2(x_2)\cdot\psi_2(x_1)\right)
    \label{sym2}
\end{equation}
The constant a drops out, and since the dot product is not order dependent: 
\begin{align}
    \psi_1(x_1)\cdot\psi_1(x_2) = \psi_1(x_2)\cdot\psi_1(x_1) \notag \\
    \psi_2(x_1)\cdot\psi_2(x_2) = \psi_2(x_2)\cdot\psi_2(x_1)
\end{align}
eqn.\ref{sym2} is true and symmetry holds. 

Positive SemiDefinite: 
\begin{equation}
    v^T K v \geq 0, \forall v\neq 0
\end{equation}

\begin{equation}
    v^T K v = v^T aK_1K_2 v
\end{equation}

Since K is symmetric, 
\begin{align}
    v^T aK_1K_2 v =& a \sum_i\sum_jv_iv_j (\psi_1(x_i) \cdot \psi_1(x_j))(\psi_2(x_i) \cdot \psi_2(x_j)) \notag \\
    =& a \sum_i\sum_jv_iv_j (\psi_1(x_i) \cdot \psi_1(x_j)) \sum_i\sum_jv_iv_j(\psi_2(x_i) \cdot \psi_2(x_j)) \notag \\
    =& a \left(\sum_iv_i\psi_1(x_i)\cdot\sum_jv_j\psi_1(x_j)\right) \left(\sum_iv_i\psi_2(x_i)\cdot\sum_jv_j\psi_2(x_j)\right) \notag \\
    =& a \parallel\sum_iv_i\psi_1(x_i)\parallel_2^2 \parallel\sum_iv_i\psi_2(x_i)\cdot\sum_jv_j\psi_2(x_j)\parallel_2^2
\end{align}

Hence
\begin{equation}
    a \parallel\sum_iv_i\psi_1(x_i)\parallel_2^2 \parallel\sum_iv_i\psi_2(x_i)\cdot\sum_jv_j\psi_2(x_j)\parallel_2^2 \geq 0
\end{equation}


\pagebreak
\setcounter{equation}{0}
2) For $K(x,y) = (x\cdot y + c)^3$

i)  
\begin{align}
   K(x,y) =& (x \cdot y)^3  + 3c(x \cdot y)^2 + 3c^2(x \cdot y) + c^3 \notag \\
   =& \left(\sum_i x_iy_i\right)\left(\sum_i x_iy_i\right)\left(\sum_i x_iy_i\right) \notag \\
   &+ 3c\left(\sum_i x_iy_i\right)\left(\sum_i x_iy_i\right) + 3c^2\left(\sum_i x_iy_i\right) + c^3
\end{align}

\begin{align}
   K(x,y) =& \left(\sum_{i,j,k} x_ix_jx_ky_iy_jy_k\right) \notag \\
   &+ 3c\left(\sum_{i,j} x_ix_jy_iy_j\right) + 3c^2\left(\sum_i x_iy_i\right) + c^3
\end{align}

\begin{align}
    K(x,y) =& \psi(x)\psi(y) \notag \\
    \psi(x) =& [x_n^3,x_nx_{n-1}^2,x_nx_{n-1}x_{n-2},...x_1^3, \notag \\
    &\sqrt{3c}x_n^2,...\sqrt{3c}x_ix_j...\sqrt{3c}x_1^2, \notag \\
    &\sqrt{3c^2}x_n,...\sqrt{3c^2}x_1,\sqrt{c^3}]
\end{align}

ii)

symmetry: 
\begin{equation}
    K(x,y) = K(y,x) 
\end{equation}

\begin{equation}
    (x\cdot y + c)^3 = (y\cdot x + c)^3
\end{equation}
Since the dot product is not order specific, symmetry holds! 

Positive SemiDefinite: 
\begin{equation}
    v^T K v \geq 0, \forall v\neq 0
\end{equation}

since K is symmetric: 

\begin{align}
    v^T K v =& \sum_i \sum_j v_i v_j (x_i \cdot x_j + c^3)  \notag \\
    =& \sum_i \sum_j v_i v_j (x_i \cdot x_j) + c^3\sum_i \sum_j v_i v_j  \notag \\
    =& \parallel\sum_i v_ix_i\parallel_2^2 + c^3(\sum_i v_i) ^2 
\end{align}

for all positive values of c
\begin{equation}
    \parallel\sum_i v_ix_i\parallel_2^2 + c^3(\sum_i v_i) ^2 \geq 0
\end{equation}

so K is positive semidefinite

\pagebreak
\setcounter{equation}{0}
3) For $K(x,y) = \psi(x)\cdot \psi(y)$, then

for symmetry: 
\begin{equation}
    K(x,y) = K(y,x) 
\end{equation}

\begin{equation}
    \psi(x)\cdot \psi(y) = \psi(y)\cdot \psi(x)
\end{equation}

Which is trivially true, so K is always symetric

for positive semidefinite: 
\begin{equation}
    v^T K v \geq 0, \forall v\neq 0
\end{equation}

Since K is symmetric, 
\begin{equation}
    v^T K v = \sum_i \sum_j v_iv_j\psi(x_i)\cdot \psi(x_j)
\end{equation}

\begin{equation}
    \sum_i \sum_j v_iv_j\psi(x_i)\cdot \psi(x_j) = \sum_i v_i\psi(x_i) \cdot \sum_j v_j\psi(x_j) = \parallel \sum_i v_ix_i \parallel_2^2
\end{equation}

\begin{equation}
    \parallel \sum_i v_ix_i \parallel_2^2 \geq 0
\end{equation}

Which is trivially true, so K is positive semidefinite




\pagebreak
\setcounter{equation}{0}
4) For Naive Bayes with all continuous attribues, $P(x_i|y) = N(\mu_{i|y},\sigma_{i|y})$.

From lecture notes in class: 
\begin{equation}
    ln(\frac{p(y=1)}{p(y=0)} ) + \sum_i ln( \frac{p(x_i|y=1)}{p(x_i|y=0)} )
\end{equation}

That means that $ln( \frac{p(x_i|y=1)}{p(x_i|y=0)} )$ becomes the ratio of 
two normal distributions... 





\pagebreak
\setcounter{equation}{0}
5) You would first split the feature set of the training set
into two: those features with complete data and those with incomplete data. 

You could simply ignore all of the features from the second set and only fit on the first. But 
this would be ignoring data that you have. Instead, you could treat the the features from 
the unknown set as hidden data and apply the EM algorithm to impute the missing data. 

6) Plotting the dataset indicates that there are two islands of positive quantities, 
one centered around the point (4,8) and the other around the point (0,4). 

This indicates that there is a missing feature in all of the data sets, which 
would indicate if the data was part of set 1 or set 2. 

So I would modify this training process with EM, adding in two new hidden features 
for each case (the x and y locations of the center of each group). Those two 
features would, after converging to solution have to be either (4,8) or (0,4) 
for all the training cases. 

\end{document}