\documentclass[12pt]{article}
\usepackage{amsmath}
\title{EECS 440: HW 4}
\author{Justin Gray}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\usepackage{graphicx}
\usepackage{float}

\begin{document}

\maketitle
\setcounter{equation}{0}
1) The concept of over fitting indicates that your learned concept too closely 
matches your example set, at the expense of worse predictions for your testing set. 
This is obviously a bad idea due to decreased accuracy of prediction. The whole point 
of the learning process is to produce some model that can provide useful information 
about inputs outside the test set. You already know the class labels of the test data, 
so predictions of those labels are useless other than for the machine learning process 
itself. 

So, assuming that you hold "over fitting" of the data to mean essentially a reduction in 
accuracy over your test/validation data, then it does not seem that there would be any 
good reason to over fit the data ever. You would never want to decrease overall accuracy 
on purpose. 

2) By repeating the process some large number, N, times, person X effectively produces an
approximation of an n-fold cross validation procedure. The n-fold cross validation 
procedure is an effective check against over fitting and a good way to measure the effectiveness 
of a learned concept. Approximating this process with a stochastic N tests of random splits of n 
sizes sounds like a reasonable empirical approximation. 

However, there are some challenges associated with splitting the set up completely randomly. 
Firstly, there is a chance that some splits will have a disproportionate number of one class
label or the other. On average, for large N, this will not be a problem though. Another issue 
to consider is that for a given set, and split size n, there is a finite number of splits which are possible. 
Running N random tests is only a good idea if N is smaller than simply enumerating the possibilities and checking them. 

3) Since the ROC of A dominates that of B, it means that the $TP rate(A) \geq TP rate(B)$
for any given FP rate. Precision is defined as $\frac{TP}{TP+FP}$. Thus, 
for any FP Rate, the precision of A must be higher than that of B. However, since 
recall is a function of the false negative rate, and you can not say anything about this 
as a function of the FP rate, then it is difficult to infer anything about the shape of the
precision-recall graph. 

4) The ROC graphs is monotonically increasing because as you lower the threshold for considering 
a result positive, you will always improve recall (or at least hold it constant). Recall is
defined as $\frac{TP}{TP+FN}$. As you lower the threshold, you increasingly count more 
results as positive which will increase your TP rate (or at least hold it constant). But you 
will never increase your FN rate, you can only decrease it as more results are counted positive. 
So as FP rate goes up, for an ROC graph, you will always move recall closer to 1 (or at least stay 
constant). 

\pagebreak
\setcounter{equation}{0}
5)Cross entropy is defined as 
\begin{equation}
    L(p,\hat{p}) = - \left( \sum_i p_i log\left(\hat{p_i}\right) + (1-p_i)log\left(1-\hat{p_i}\right)\right)
\end{equation}
To include a weight decay term, the modifed loss function becomes: 
\begin{equation}
    L_{OC}(p,\hat{p};w) = L(p,\hat{p};w) + \gamma\sum_i\sum_j w_{ij}^2
\end{equation}

%For the output layer, assuming sigmoid activation functions for $h(u)$, then 
%\begin{equation}
%    \frac{dh}{du} = \frac{e^{-u}}{(1+e^{-u})^2} = h(u)(1-h(u))
%\end{equation}

For the $y_jth$ node, the loss function is
\begin{equation}
    L_{OC}(p_j,\hat{p_j}) = L\left(p_j,\hat{p_j}\right) + \gamma\sum_i\sum_j w_{ij}^2
\end{equation}
%Noting that: 
%\begin{equation}
%    \frac{\partial L}{\partial w_{ji}} = 
%    \frac{\partial L}{\partial n_j}\frac{\partial n_j}{\partial w_{ji}} = 
%    \frac{\partial L}{\partial n_j} x_{ji}
%\end{equation}

So the derivative of the loss w.r.t. $n_j$ is: 
\begin{equation}
    \frac{\partial L_{OC}}{\partial n_j} = \frac{\partial}{\partial n_j}L\left(p_j,\hat{p_j})\right)
    +\frac{\partial}{\partial n_j}\gamma\sum_i\sum_j w_{ij}^2 
\end{equation}
Note that: 
\begin{equation}
    \frac{\partial}{\partial n_j}\gamma\sum_i\sum_j w_{ij}^2 = 0 
\end{equation}
so 
\begin{equation}
    \frac{\partial L_{OC}}{\partial n_j} = \frac{\partial}{\partial n_j}L\left(p_j,\hat{p_j}\right)
\end{equation}

\begin{align}
    \frac{\partial}{\partial n_j}L(p,\hat{p}) =& -\frac{\partial}{\partial n_j}\sum_i p_i log\left(\hat{p_i}\right) 
    - \frac{\partial}{\partial n_j}\sum_i(1-p_i)log\left(1-\hat{p_i}\right) \notag \\
    =& -\sum_i p_i  \frac{\partial}{\partial n_j} log\left(\hat{p_i}\right) 
    - \sum_i(1-p_i) \frac{\partial}{\partial n_j} log\left(1-\hat{p_i}\right)
\end{align}
noting that: 
\begin{equation}
    \frac{d}{dx}log(x) = \frac{1}{x ln(10)}
\end{equation}
eqn. 8 becomes: 
\begin{equation}
    \frac{\partial}{\partial n_j}L(p,\hat{p}) = -\sum_i\frac{p_i}{ln(10)}  \frac{1}{\hat{p_i}}\frac{\partial \hat{p_i}}{\partial n_j}
    + \sum_i\frac{(1-p_i)}{ln(10)} \frac{1}{1-\hat{p_i}}\frac{\partial \hat{p_i}}{\partial n_j}
\end{equation}
for any $i \neq j$: 
\begin{equation}
    \frac{\partial \hat{p_i}}{\partial n_j} = 0 
\end{equation}
So: 
\begin{equation}
    \frac{\partial}{\partial n_j}L(p,\hat{p}) = -\frac{p_j}{ln(10)} \frac{1}{\hat{p_j}}\frac{\partial \hat{p_j}}{\partial n_j}
    + \frac{(1-p_j)}{ln(10)} \frac{1}{1-\hat{p_j}}\frac{\partial \hat{p_j}}{\partial n_j}
\end{equation}

We're looking for $\frac{\partial L}{\partial w_{ji}}$
\begin{equation}
    \frac{\partial L}{\partial w_{ji}} = \frac{\partial L}{\partial n_j}\frac{\partial n_j}{\partial w_{ji}} = \frac{\partial L}{\partial n_j}x_{ji}
\end{equation}

So, combining eqns. 11 and 12: 
\begin{equation}
    \frac{\partial L}{\partial w_{ji}} = \left( -\frac{p_j}{ln(10)} \frac{1}{\hat{p_j}}\frac{\partial \hat{p_j}}{\partial n_j}
    + \frac{(1-p_j)}{ln(10)} \frac{1}{1-\hat{p_j}}\frac{\partial \hat{p_j}}{\partial n_j} \right) x_{ji}
\end{equation}

if using sigmoid activation functions, then 
\begin{equation}
    \frac{\partial \hat{p_j}}{\partial n_j} = \frac{dh}{dn_j} = h(n_j)(1-h(n_j)) = \hat{p_j}(1-\hat{p_j})
\end{equation}
which simplifies eqn. 13 to: 
\begin{equation}
    \frac{\partial L}{\partial w_{ji}} = \left(\frac{(1-p_j)\hat{p_j}}{ln(10)}
     -\frac{p_j(1-\hat{p_j})}{ln(10)} \right) x_{ji}
\end{equation}


\end{document}