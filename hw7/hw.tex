\documentclass[12pt]{article}
\usepackage{amsmath}
\title{EECS 440: HW 7}
\author{Justin Gray}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\usepackage{graphicx}
\usepackage{float}

\usepackage{minted}

\begin{document}

\maketitle
\setcounter{equation}{0}
1) Each network has one trivial case where all variables are conditionally 
independent of eachother. n variables will translate into n nodes in a baysian 
network. 

For any collection of n nodes, there must be $n^2-n-1$ possible different
directed graphs combinations, assuming a fixed order for the nodes. Then, 
reorganizing the nodes will give you n different versions of each combination. 

So the total number of possible networks is $1+ n(n^2-n-1)$. 

\pagebreak
\setcounter{equation}{0}
2) 
we know that
\begin{equation}
    \pi_{t+1}(x^{\prime}) = \sum_x \pi_t(x)T(x,x^{\prime}) 
\end{equation}

for a markov chain with detailed balance: 
\begin{equation}
    \pi_t(x)T(x,x^{\prime}) =  \pi_t(x^{\prime})T(x^{\prime},x)
\end{equation}

combining equations 1 and 2 yeilds: 

\begin{equation}
    \pi_{t+1}(x^{\prime}) = \sum_x \pi_t(x^{\prime})T(x^{\prime},x) 
\end{equation}

Since $\pi_t(x^{\prime})$ is independent of x, it can move outside the summation

\begin{equation}
    \pi_{t+1}(x^{\prime}) = \pi_t(x^{\prime}) \sum_x T(x^{\prime},x) 
\end{equation}

Notice that $\sum_x T(x^{\prime},x)$ must equal 1, so 

\begin{equation}
    \pi_{t+1}(x') = \pi_{t}(x')
\end{equation}

And the chain is stationary



\pagebreak
\setcounter{equation}{0}
3) The transition matrix resulting from the directed graph. 

$
A = \left( \begin{array}{ccc}
.4 & .4 & .2 \\
.4 & .6 & 0. \\
.8 & 0  & .2 \end{array} \right) 
$

The stationary distribution for this markov chain is the vector, $v$, such that. 

\begin{equation}
   Av = v
\end{equation}

Thus $v$ is the eigen vector of the transition matrix, for an eigen value of 1. 

\begin{equation}
    v = \left(\begin{array}{c}
    .444444444 \\
    .444444444 \\
    .111111111 \end{array} \right)
\end{equation}

To test if this disribution is balanced
\begin{equation}
     \pi(x)T(x,x') = \pi(x')T(x',x)
\end{equation}
for all possible pairs of x, x'
for x1,x2: 
\begin{equation}
     .4444*.4 = .4444*.4
\end{equation}

for x1,x3: 
\begin{equation}
     .4444*.2 = .1111*.8 = .088888
\end{equation}

for x2,x3: 
\begin{equation}
     .4444*0 = .1111*0 = 0
\end{equation}

note that for any pair composed of the same state twice, the above is trivially satisfied. 

So this distribution does put the markov chain in detailed balance. 

Using such a markov chain to perform direct inference on a bayesian network would work, as the result 
gives you the probabilities of the unknown variables. However, the challenge here is that to derive such a 
markov chain you need to make certain assumptions about the data (evidence variables). So you could only 
perform inference on a given bayesian network if that inference allowed (or assumed) the same evidence 
originally used to derive the chain. Otherwise you would need to derive a new chain for each new set of 
evidence variables. 

\pagebreak
\setcounter{equation}{0}
4) Using bayes theorem:
\begin{equation}
    Pr(X|MB(X))Pr(MB(X)) = Pr(X,MB(X))
\end{equation}

Since we know that $Pr(MB(X))$ will be fixed relative to X, we ignore it yeilding: 

\begin{equation}
    Pr(X|MB(X)) \propto Pr(X,MB(X))
\end{equation}

Since we know that given the MB(X) that X is conditionally independent from the rest of 
the baysian network, and the $MB(X) = Pa(X) + C(X) + Pa(C(X))$, then: 

\begin{equation}
    Pr(X|MB(X)) \propto Pr(X|Pa(X))\prod_c P(C|Pa(C))
\end{equation}


\pagebreak
\setcounter{equation}{0}
5)
i) \begin{equation}
  Pr(R|-c,w) = \alpha P(R,-c,w)=\alpha Pr(-c)Pr(R|-c)\sum_x Pr(S|-c) Pr(W|S,R)
\end{equation}

For the case where it did rain: 
\begin{equation}
  Pr(R|-c,w) = \alpha .5*.2*(.5*.9+.5*.6) = \alpha .075
\end{equation}

For the case where it did not rain: 
\begin{equation}
  Pr(R|-c,w) = \alpha .5*.8*(.5*.8+.5*.1) = \alpha .180
\end{equation}

\begin{equation}
P(R|-c,w) =  \alpha<.075,.180> = <.3,.7>
\end{equation}

So the probability that it did rain is 0.3.

ii) Using rejection sampling with 10,000 samples, you get a probability of 
.2912

{\scriptsize \inputminted{python}{rejection_sampling.py} }

iii) Using gibbs sampling with 10,000 samples (measuring on the last 5000) you get a probability of 
.2924

{\scriptsize \inputminted{python}{gibbs.py} }

\end{document}